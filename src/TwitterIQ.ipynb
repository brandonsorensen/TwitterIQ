{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PostingNode(object):\n",
    "    \"\"\"\n",
    "    A node that contains a reference to the postings list and the \n",
    "    length of that posting list, or the word's frequency\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, postings_list: list, freq: int = 1):\n",
    "        self.postings_list = postings_list\n",
    "        self.freq = freq\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'[Frequency: {self.freq}, {self.postings_list[:5]}'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "    def __ne__(self, other) -> bool:\n",
    "        return self.freq == other.freq\n",
    "\n",
    "    def __gt__(self, other) -> bool:\n",
    "        return self.freq > other.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from typing import *\n",
    "from nltk.corpus import stopwords\n",
    "from emoji import UNICODE_EMOJI\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "class TwitterIQ(dict):\n",
    "    \"\"\"\n",
    "    This class creates an inverted index of a .csv file\n",
    "    that contains information and content from a number of tweets.\n",
    "\n",
    "    This class inherits from the default dictionary class in Python.\n",
    "    This was done primarily to utilize the __missing__ function,\n",
    "    which serves to make the code a bit more Pythonic. The class loads\n",
    "    each token individually and uses the __missing__ method to determine\n",
    "    whether it is already in the dictionary (self). In the case the it\n",
    "    is, the id of the particular tweet/doc is added to the relevant\n",
    "    postings list.\n",
    "\n",
    "    Query methods are detailed in their respective methods' docstrings.\n",
    "\n",
    "    Attributes:\n",
    "        all_postings: List containing ALL postings lists\n",
    "        tweet_content_dict: Dictionary whose keys are twitter_ids\n",
    "            pointing to the content of the tokenized tweets.\n",
    "        __current_tweet_id: ID of the doc/tweet that is currently\n",
    "            being iterated over. This is used by the __missing__ method\n",
    "            to propery organize the dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    STOP_WORDS = (stopwords.words('english') + stopwords.words('german'))\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"\n",
    "        Initializes by walking through each token and creating an\n",
    "        inverted index as detailed above.\n",
    "\n",
    "        :param str path: the path to the string\n",
    "        \"\"\"\n",
    "        self.all_postings = []\n",
    "        self.tweet_content_dict = {}\n",
    "        self.__current_tweet_id = None\n",
    "\n",
    "        with open(path, 'r') as corpus:\n",
    "            # combs through each doc/tweet individually\n",
    "            for doc in corpus:\n",
    "                tokenized_doc = doc.split()      # tokens by white space \n",
    "                tweet_id = tokenized_doc[3]\n",
    "                tweet_content = tokenized_doc[5:] # what the user wrote\n",
    "                # stores the tokenized content in a dict identified by the tweet_id\n",
    "                self.tweet_content_dict[tweet_id] = tweet_content\n",
    "                self.__current_tweet_id = tweet_id\n",
    "\n",
    "                for token in tweet_content:\n",
    "                    self.__index_tokens(tweet_content)\n",
    "\n",
    "    def __missing__(self, token: str):\n",
    "        \"\"\"\n",
    "        If entry is missing, add posting to all_postings, then creates\n",
    "        a PostingNode and sets the entry to equal that value.\n",
    "        \n",
    "        :param str token: The token to be added to the dictionary\n",
    "        :return: posting\n",
    "        \"\"\"\n",
    "        self.all_postings.append([self.__current_tweet_id])\n",
    "        self[token] = PostingNode(self.all_postings[-1])\n",
    "        return self[token]\n",
    "\n",
    "    def __index_tokens(self, tweet_content: list) -> None:\n",
    "        \"\"\"\n",
    "        Walks the the list of tokens from each tweet. If a token\n",
    "        is not clean, it exits without adding an entry. It then\n",
    "        adds to (or uses the __missing__ method to create) the\n",
    "        dictionary (self) entry for each token. It then adds to the\n",
    "        posting list of each token.\n",
    "\n",
    "        :param list tweet_content: a list of tokens from tweet\n",
    "        :returns: None\n",
    "        \"\"\"\n",
    "        tweet_id = self.__current_tweet_id\n",
    "        for token in tweet_content:\n",
    "            if not self.__clean(token):\n",
    "                return\n",
    "            \n",
    "            # creates entry or assigns posting_node to existing one\n",
    "            posting_node = self[token]\n",
    "            if tweet_id not in posting_node.postings_list:\n",
    "                # adds to end of posting list and increments freq\n",
    "                posting_node.postings_list.append(tweet_id)\n",
    "                posting_node.freq += 1\n",
    "\n",
    "    def __clean(self, token: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes stop words for English and German and makes\n",
    "        all tokens lowercase.\n",
    "\n",
    "        :param str token: the token to be cleaned\n",
    "        :return: The token if it can be cleaned, None if not\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "       \n",
    "        if token in punctuation:\n",
    "            return\n",
    "        \n",
    "        if token in UNICODE_EMOJI:\n",
    "            return \n",
    "        \n",
    "        token = token.lower()\n",
    "\n",
    "        if token in TwitterIQ.STOP_WORDS:\n",
    "            return\n",
    "\n",
    "        return token\n",
    "\n",
    "    def get_tokens_from_tweet(self, tweet_id: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the tokens from a specific tweet/doc ID\n",
    "\n",
    "        :param int tweet_id: the tweet to be returned\n",
    "        :return: a list of tokens from a tweet\n",
    "        :rtype: List[str]\n",
    "        \"\"\"\n",
    "        return self.tweet_content_dict[tweet_id]\n",
    "\n",
    "    def get_most_freq_words(self, limit: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the words with the three largest frequencies.\n",
    "        \n",
    "        :param int limit: the optional n number of words to return\n",
    "        :return: the most frequently used words in the corpus\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        return heapq.nlargest(limit, self, key=self.get)\n",
    "\n",
    "    def query(self, term1: str, term2: str = None) -> List[int]:\n",
    "        \"\"\"\n",
    "        Gets the postings list for a term or the intersection or the\n",
    "        posting lists of two different terms.\n",
    "        \n",
    "        :param str term1: the first (or only) string to query\n",
    "        :param str term2: the optione 2nd string to intersect with\n",
    "        :return: the positings list or intersection of two\n",
    "        \"\"\"\n",
    "        if term2 is None:\n",
    "            return self[term1].postings_list\n",
    "        else:\n",
    "            return list(set(self.query(term1)) & set(self.query(term2)))\n",
    "    \n",
    "    def print_query(self, term1: str, term2: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Pretty prints the query method.\n",
    "        \n",
    "        :param str term1: the first term to query\n",
    "        :param str term2: the optional second term to intersect with\n",
    "        \"\"\"\n",
    "        for tweet_id in self.query(term1, term2):\n",
    "            print(f'{tweet_id}:', ' '.join(\n",
    "                self.tweet_content_dict[tweet_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = TwitterIQ('tweets.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Frequency: 5, ['960297299950952448', '979860993898672128', '990376770200272902', '982758768193822720', '980220498440347648']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer['schlafen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147416"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
